{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Supervised Learning with categorical output\n",
    "\n",
    "- Typical Classification problem with 2, 3, 4 (or more) outputs.\n",
    "- Most of the time the output consists of binary (male/female, spam/nospam,yes/no)\n",
    "- Sometime, there are more than binary output: dog/cat/mouse, red/green/yellow.\n",
    "\n",
    "In this category, we are going to use 2 existing dataset from [sklearn](https://scikit-learn.org/stable/datasets.html):\n",
    "- [Breast Cancer Wisconsine](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset) data for Binary output\n",
    "                                     - [Iris plant](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) data for multiple (3) output.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Logistic Regression for binary output\n",
    "\n",
    "- Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).\n",
    "- Typical binary classification: True/False, Yes/No, Pass/Fail, Spam/No Spam, Male/Female\n",
    "                                                                        - Unlike linear regression, the prediction for the output is transformed using a non-linear function called the logistic function.\n",
    "- The standard logistic function has formulation:\n",
    "\n",
    "    ![image](https://user-images.githubusercontent.com/43855029/114233181-f7dcbb80-994a-11eb-9c89-58d7802d6b49.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114233189-fb704280-994a-11eb-9019-8355f5337b37.png)\n",
    "\n",
    "In this example, we load a sample dataset called [Breast Cancer Wisconsine](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Breast Cancer Wisconsine data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print(\"There are\", X.shape[1], \" Predictors: \", data.feature_names)\n",
    "print(\"The output has 2 values: \", data.target_names)\n",
    "print(\"Total size of data is \", X.shape[0], \" rows\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that there are 30 input data representing the shape and size of 569 tumours.\n",
    "Base on that, the tumour can be considered _malignant_ or _benign_ (0 or 1 as in number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partitioning Data to train/test:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model using Logistic Regression\n",
    "For simplicity, we will use all of the predictors for the regression:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LogReg = LogisticRegression(solver='newton-cg').fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model output:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model_LogReg.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"The accuracy score is %1.3f\" % metrics.accuracy_score(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We retrieve the **accuracy = 0.965** using all predictors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute AUC-ROC and plot curve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "lr_probs = model_LogReg.predict_proba(X_test)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = np.zeros(len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs[:,1])\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs[:,1])\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/153662934-d4c5929f-72cf-43b8-8b1f-085d315022e7.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An alternative way to plot AUC-ROC curve, using additional toolbox [\"scikit-plot\"](https://scikit-plot.readthedocs.io/en/stable/)\n",
    "Use this command:\n",
    "```\n",
    " pip install scikit-plot\n",
    "```\n",
    "\n",
    "The shorter code for using this library:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_roc(y_test, lr_probs)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/153663219-f27aad2b-b76d-4abf-a093-0a433e79bd28.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Classification problem with more than 3 outputs\n",
    "\n",
    "Here we use [Iris plant](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) data for multiple (3) output.\n",
    "\n",
    "### Import data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(\"There are\", X.shape[1], \" Predictors: \", data.feature_names)\n",
    "print(\"The output has 3 values: \", data.target_names)\n",
    "print(\"Total size of data is \", X.shape[0], \" rows\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "- We can see that there are 4 input data representing the petal/sepal width and length of 3 different kind of iris flowers.\n",
    "- Base on that, the iris plants can be classified as 'setosa' 'versicolor' 'virginica'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Partitioning Data to train/test:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train model using Linear Discriminant Analysis (LDA):\n",
    "For simplicity, we will use all of the predictors for the regression:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model_LDA = LinearDiscriminantAnalysis().fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model output:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The accuracy score is %1.3f\" % model_LDA.score(X_test,y_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA can be used for both binary and more categorical output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercise: create an LDA model to predict the breast cancer Wisconsine data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Other Algorithms\n",
    "\n",
    "There are many other algorithms that work well for both classification and regression data such as Decision Tree, RandomForest, Bagging/Boosting.\n",
    "Very similar to chapter 5, the following model should be loaded:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercise: create a Random Forest model to predict the iris flower data using the same method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 Principal Component Analysis\n",
    "- Handy with large data\n",
    "- Where many variables correlate with one another, they will all contribute strongly to the same principal component\n",
    "- Each principal component sums up a certain percentage of the total variation in the dataset\n",
    "- More Principal Components, more summarization of the original data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.1 PCA formulation\n",
    "- For example, we have 3 data sets: `X, Y, Z`\n",
    "- We need to compute the covariance matrix **M** for the 3 data set:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114459677-d67c0980-9bae-11eb-85b2-758a98f0cd29.png)\n",
    "\n",
    "in which, the covariance value between 2 data sets can be computed as:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114459740-ea277000-9bae-11eb-9259-8ef1b233c0fa.png)\n",
    "\n",
    "- For the Covariance matrix **M**, we will find **m** eigenvectors and **m** eigenvalues\n",
    "\n",
    "```\n",
    "- Given mxm matrix, we can find m eigenvectors and m eigenvalues\n",
    "- Eigenvectors can only be found for square matrix.\n",
    "    - Not every square matrix has eigenvectors\n",
    "- A square matrix A and its transpose have the same eigenvalues but different eigenvectors\n",
    "- The eigenvalues of a diagonal or triangular matrix are its diagonal elements.\n",
    "- Eigenvectors of a matrix A with distinct eigenvalues are linearly independent.\n",
    "```\n",
    "\n",
    "**Eigenvector with the largest eigenvalue forms the first principal component of the data set\n",
    "… and so on …***\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Implementation\n",
    "\n",
    "Here we gonna use the breast cancer Wisconsine data set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.6,random_state=123)\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2.1 Compute PCA using sklearn:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "PCs = pca.fit_transform(X_train_scaled)\n",
    "PCs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the shape of PC's are [341,30], which has the same 30 inputs/principal components as in the original data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2.2 Explained Variance\n",
    "\n",
    "The explained variance tells you how much information (variance) can be attributed to each of the principal components."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "print(\"The first 4 components represent %1.3f\" % pca.explained_variance_ratio_[0:4].sum(), \" total variance\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since using only 4 PCs, it is able to represent 30 PCs in the entire data, therefore, we use this 4 PCs to construct the ML model using K-Nearest Neighbors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2.3 Application of PCA model in Machine Learning:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score as acc_score\n",
    "pca = PCA(n_components=4) #We choose number of principal components to be 4\n",
    "\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_scaled))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test_scaled))\n",
    "X_train_pca.columns = ['PC1','PC2','PC3','PC4']\n",
    "X_test_pca.columns  = ['PC1','PC2','PC3','PC4']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use random forest to train model\n",
    "model_RF = KNeighborsClassifier().fit(X_train_pca, y_train)\n",
    "y_pred_RF = model_RF.predict(X_test_pca)\n",
    "print(\"The accuracy score is %1.3f\" % acc_score(y_test,y_pred_RF))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting the testing result with indicator of Wrong prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "targets = np.unique(y_pred_KNN)\n",
    "colors = ['r', 'g']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indp = y_pred_KNN == target\n",
    "ax.scatter(X_test_pca.loc[indp, 'PC1'], X_test_pca.loc[indp, 'PC2'],c = color)\n",
    "\n",
    "# Ploting the Wrong Prediction\n",
    "ind = y_pred_KNN!=np.array(y_test)\n",
    "ax.scatter(X_test_pca.loc[ind, 'PC1'],X_test_pca.loc[ind, 'PC2'],c = 'black')\n",
    "\n",
    "#axis control\n",
    "ax.legend(['malignant','benign','Wrong Prediction'])\n",
    "ax.set_title(\"Testing set from KNN using PCA 4 components\")\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/153672409-2bcefb86-5bf2-497f-b1ca-00af35b776d1.png)\n",
    "\n",
    "As seen, there are 4 points that were wrongly identified"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 Neural Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1 The Neural Network of a brain\n",
    "\n",
    "- Neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n",
    "- Neuron is a basic unit in a nervous system and is the most important component of the brain.\n",
    "- In each Neuron, there is a cell body (node), dendrite (input signal) and axon (output signal to other neuron).\n",
    "- If a Neuron received enough signal, it is then activated to decide whether or not it should transmitt the signal to other neuron or not.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114472746-da188c00-9bc0-11eb-913c-9dcd14f872ac.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2 Neural Network in Machine Learning:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114472756-dd137c80-9bc0-11eb-863d-7c4d054efa89.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.3 Formulation of Neural Network:\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114472776-e997d500-9bc0-11eb-9f70-450389c912df.png)\n",
    "\n",
    "Here:\n",
    "- x1,x2....xn are input variables.\n",
    "- w1,w2....wn are weights of respective inputs.\n",
    "- b is the bias, which is summed with the weighted inputs to form the net inputs.\n",
    "\n",
    "In which:\n",
    "- Bias and weights are both adjustable parameters of the neuron.\n",
    "- Parameters are adjusted using some learning rules.\n",
    "- The output of a neuron can range from -inf to +inf. As the neuron doesn’t know the boundary, so we need a mapping mechanism between the input and output of the neuron. This mechanism of mapping inputs to output is known as Activation Function.\n",
    "\n",
    "**Activation functions:**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114575672-6752f380-9c48-11eb-8d53-c78d052cdf17.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.4 Multi-Layer Perceptron (MLP)\n",
    "\n",
    "**Multi-layer Perceptron (MLP)** is a supervised learning algorithm.\n",
    "Given a set of features `X = x1, x2, ... xm`, and target `y`, MLP can learn a non-linear function approximator for either classification or regression.\n",
    "\n",
    "Between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure below shows a one hidden layer MLP with scalar output.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114472972-51e6b680-9bc1-11eb-9e78-90ec739844ee.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114575549-48546180-9c48-11eb-8c9c-c5eac3180df1.png)\n",
    "\n",
    "**The advantages of Multi-layer Perceptron:**\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "**The disadvantages of Multi-layer Perceptron:**\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 8.5 Type of Neural Network Multi-Layer Perceptron in sklearn\n",
    "Similar to previous Machine Learning model, there are 2 main types of MLP in sklearn, depending on the model output:\n",
    "- MLPClassifier: for Classification problem\n",
    "    - MLPRegressor: for Regression problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.6 Implementation with Classification problem\n",
    "\n",
    "Here we use **Breast Cancer Wisconsine** data for Classification problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Class **MLPClassifier** implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.\n",
    "There are lots of parameters in MLPClassifier:\n",
    "- **hidden_layer_sizes** which is the number of hidden layers and neurons for each layer. Default=`(100,)`\n",
    "for example `hidden_layer_sizes=(100,)` means there is 1 hidden layers used, with 100 neurons.\n",
    "for example `hidden_layer_sizes=(50,20)` means there are 2 hidden layers used, the first layer has 50 neuron and the second has 20 neurons.\n",
    "- **solver** `lbfgs, sgd, adam`. Default=`adam`\n",
    "- **activation** `identity, logistic, tanh, relu`. Default='relu`\n",
    "\n",
    "More information can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model_NN = MLPClassifier(hidden_layer_sizes = (50,20),solver='lbfgs',activation='relu',random_state=123).fit(X_train_scaled, y_train)\n",
    "model_NN.score(X_test_scaled,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.7 Implementation with Regression problem\n",
    "- Class **MLPRegressor** implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function.\n",
    "- Therefore, it uses the square error as the loss function, and the output is a set of continuous values.\n",
    "\n",
    "Here we use **california housing** data from Regression espisode:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Predictors/Input:\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "\n",
    "# Predictand/output:\n",
    "y = pd.DataFrame(data.target,columns=data.target_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit **MLPRegressor** model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "model_NN = MLPRegressor(hidden_layer_sizes = (10,5),solver='lbfgs',activation='tanh',max_iter=1000).fit(X_train,y_train)\n",
    "model_NN.score(X_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.8 Tips on using MLP\n",
    "- Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data.\n",
    "- Empirically, we observed that **L-BFGS** converges faster and with better solutions on small datasets. For relatively large datasets, however, **Adam** is very robust. It usually converges quickly and gives pretty good performance. **SGD** with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.\n",
    "- Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.\n",
    "- The loss function for Classifier is **Cross-Entropy** while for Regression is **Square-Error**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.9. Notes\n",
    "- There are many other NN algorithms which will be introduced in the Deep Learning class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9 Unsupervised Learning\n",
    "\n",
    "- No labels are given to the learning algorithm leaving it on its own to find structure in its input.\n",
    "- Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\n",
    "- Used when no feature output data\n",
    "- Often used for clustering data\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114584282-82c1fc80-9c50-11eb-9342-41e5592e7b67.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114584314-89507400-9c50-11eb-9c54-5a589075fd48.png)\n",
    "\n",
    "**Typical method:**\n",
    "\n",
    "```\n",
    "K-means clustering\n",
    "Hierarchical clustering\n",
    "Ward clustering\n",
    "Partition Around Median (PAM)\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 9.1 K-means clustering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.1.1 Explanation of K-means clustering method:\n",
    "- Given a set of data, we choose K=2 clusters to be splited:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114584415-a5ecac00-9c50-11eb-8919-807f83ddf23a.png)\n",
    "\n",
    "- First select 2 random centroids (denoted as red and blue X)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114584573-d16f9680-9c50-11eb-9dc4-8d918919f565.png)\n",
    "\n",
    "- Compute the distance between 2 centroid red X and blue X with all the points (for instance using Euclidean distance) and compare with each other. 2 groups are created with shorter distance to 2 centroids\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114584860-0bd93380-9c51-11eb-9afc-3bb9510e9c34.png)\n",
    "\n",
    "- Now recompute the **new** centroids of the 2 groups (using mean value of all points in the same groups):\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114585002-34f9c400-9c51-11eb-83e0-b5769abf6cd3.png)\n",
    "\n",
    "- Compute the distance between 2 **new** centroids and all the points. We have 2 new groups:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114585030-3b883b80-9c51-11eb-8f69-29f6e406e215.png)\n",
    "\n",
    "- Repeat the last 2 steps until **no more new centroids** created. The model reach equilibrium:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114585223-6b374380-9c51-11eb-8663-27474956ec61.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.1.2 Example with K=3\n",
    "![image](https://user-images.githubusercontent.com/43855029/114585361-8e61f300-9c51-11eb-965e-dc4d57e9c0eb.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114585502-b81b1a00-9c51-11eb-8015-973216b450ce.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 9.1.3. Implementation\n",
    "Here we use the iris data set with only predictors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "X = iris.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply Kmeans and plotting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_KMeans = KMeans(n_clusters=3)\n",
    "model_KMeans.fit(X)\n",
    "\n",
    "plt.scatter(X[:,2],X[:,3],c=model_KMeans.labels_)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.title('KMeans clustering with 3 clusters')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/115735833-c99ea900-a358-11eb-87d8-774efc7fa459.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.1.4 How to find optimal K values:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.1.4.1 Elbow approach\n",
    "- Similar to KNN method for supervised learning, for K-means approach, we are able to use Elbow approach to find the optimal K values.\n",
    "- The Elbow approach uses the Within-Cluster Sum of Square (WSS) to measure the compactness of the clusters:\n",
    "![image](https://user-images.githubusercontent.com/43855029/114587068-4d6ade00-9c53-11eb-932d-0de0c9edef83.png)\n",
    "\n",
    "The optimal K-values can be found from the Elbow using **method=\"wss\"**:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wss = []\n",
    "for k in range(1,10):\n",
    "    model = KMeans(n_clusters=k).fit(X)\n",
    "    wss.append(model.inertia_)\n",
    "\n",
    "plt.scatter(range(1,10),wss)\n",
    "plt.plot(range(1,10),wss)\n",
    "plt.xlabel(\"Number of Clusters k\")\n",
    "plt.ylabel(\"Within Sum of Square\")\n",
    "plt.title(\"Optimal number of clusters based on WSS Method\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/115737965-9b21cd80-a35a-11eb-9bcd-0d63e685ec0f.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.1.4.2 Gap-Statistics approach\n",
    "- Developed by Prof. Tibshirani et al in Stanford\n",
    "- Applied to any clustering method (K-means, Hierarchical)\n",
    "- Maximize the Gap function:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114586376-95d5cc00-9c52-11eb-9b71-ed330cfc50bc.png)\n",
    "\n",
    "E*n: expectation under a sample size of n from the reference distribution\n",
    "![image](https://user-images.githubusercontent.com/43855029/114586396-9b331680-9c52-11eb-9b83-955aa256e623.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114586456-af771380-9c52-11eb-9fdb-99cc8df854fb.png)\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "This version of Gap Statistics is not official. Until the moment of writing this documentation, no official Gap Statistics has been released in Python.\n",
    "We use the version from [milesgranger's github](https://github.com/milesgranger/gap_statistic)\n",
    "                         ```python\n",
    "                         pip install git+git://github.com/milesgranger/gap_statistic.git\n",
    "pip install gapstat-rs\n",
    "```\n",
    "Implement Gap-Statistics:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gap_statistic import OptimalK\n",
    "\n",
    "optimalK = OptimalK(n_jobs=1) # No parallel\n",
    "n_clusters = optimalK(X[:,1:4], cluster_array=np.arange(1, 15))\n",
    "print('Optimal clusters: ', n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot Gap-Statistics:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, linewidth=3)\n",
    "plt.scatter(optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].n_clusters,\n",
    "            optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].gap_value, s=250, c='r')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Cluster Count')\n",
    "plt.ylabel('Gap Value')\n",
    "plt.title('Gap Values by Cluster Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/115745658-a298a500-a361-11eb-8071-6af68f7eb428.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.2 Comparison between different clustering methods in sklearn:\n",
    "- This is an example from [sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)\n",
    "- The source code for image below can be found [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-download-auto-examples-cluster-plot-cluster-comparison-py)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/115748324-0f14a380-a364-11eb-8a06-6d073b4d99c4.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}