{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 What is Scikit-Learn\n",
    "![image](https://user-images.githubusercontent.com/43855029/114609814-30db9f80-9c6d-11eb-8d4e-781f578e1d79.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Scikit-learn is probably the most useful library for machine learning in Python.\n",
    "    - The sklearn library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.\n",
    "- The sklearn package contains tools for:\n",
    "\n",
    "```\n",
    "- data splitting\n",
    "- pre-processing\n",
    "- feature selection\n",
    "- model tuning using resampling\n",
    "- variable importance estimation\n",
    "as well as other functionality.\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Install sklearn\n",
    "We have installed kernel **ML_SKLN** which contains the scikit-learn package in M2. More information can be found in the setup page\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Pre-processing using sklearn\n",
    "There are several steps that we will use `sklearn` for. For preprocessing raw data, we will use `sklearn` in these tasks:\n",
    "- Preprocessing with missing values\n",
    "- Preprocessing: transform data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 Pre-processing with missing value\n",
    "- Most of the time the input data has missing values (`NA, NaN, Inf`) due to data collection issue (power, sensor, personel).\n",
    "- There are three main problems that missing data causes: missing data can introduce a substantial amount of bias, make the handling and analysis of the data more arduous, and create reductions in efficiency\n",
    "- These missing values need to be treated/cleaned before we can use because \"Garbage in => Garbage out\".\n",
    "- There are several ways to treat the missing values:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/153270189-5bf6f452-64ab-4af7-b30d-de985c8c5661.png)\n",
    "[source](https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Read in data with missing value and check the missing values:\n",
    "\n",
    "import pandas as pd\n",
    "data_df = pd.read_csv('https://raw.githubusercontent.com/vuminhtue/SMU_Machine_Learning_Python/master/data/airquality.csv')\n",
    "data_df.head()\n",
    "data_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Method 1: ignore missing values:\n",
    "Many functions in python ignore the missing values, for example, the mean & count function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_df['Ozone'].mean()\n",
    "data_df['Ozone'].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will see that the count function only prints 116 values (out of 153 values (including NA) in total) of The columns of \"Ozone\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Method 2: remove entire row with missing `NA` values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data2 = data_df.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Method 3: drop the entire column (not recommended):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data3 = data_df.drop(\"Ozone\",axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: axis = 1 (column), axis = 0 (row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Method 4: Fill NA with constant values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Often times, the missing data can be set to 0 or 1 (or any other meaningful data set in your field):\n",
    "The Following code fill the missing value with 0:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data4 = data_df.copy()\n",
    "data4.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Method 5: Fill `NA` to mean/median/max/min value\n",
    "Very similar to filling with constant value:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data5 = data_df.copy()\n",
    "data5.fillna(data5.mean(), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or using SimpleImputer function from sklearn:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "data5 = pd.DataFrame(imputer.fit_transform(data_df))\n",
    "data5.columns = data_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:**\n",
    "SimpleImputer converts missing values to **mean, median, most_frequent and constant**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Method 6: **Advanced** Use KN-based `Impute` to handle missing values\n",
    "\n",
    "In statistics, imputation is the process of replacing missing data with substituted values. Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, which may introduce bias or affect the representativeness of the results. Imputation preserves all cases by replacing missing data with an estimated value based on other available information. Once all missing values have been imputed, the data set can then be analysed using standard techniques for complete data. There have been many theories embraced by scientists to account for missing data but the majority of them introduce bias. A few of the well known attempts to deal with missing data include: hot deck and cold deck imputation; listwise and pairwise deletion; mean imputation; non-negative matrix factorization; regression imputation; last observation carried forward; stochastic imputation; and multiple imputation.\n",
    "\n",
    "`knnImpute` can also be used to fill in missing value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "data_knnimpute = pd.DataFrame(imputer.fit_transform(data_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:**\n",
    "- In addition to KNNImputer, there are **IterativeImputer** (Multivariate imputer that estimates each feature from all the others) and **MissingIndicator**(Binary indicators for missing values)\n",
    "- More information on sklearn.impute can be found [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 Pre-processing with Transforming data\n",
    "#### 2.3.2.1 Using Standardization\n",
    "![image](https://user-images.githubusercontent.com/43855029/114231774-df6ba180-9948-11eb-9c61-3d2e0d3df889.png)\n",
    "\n",
    "- Standardization comes into picture when features of input data set have large differences between their ranges, or simply when they are measured in different measurement units for example: rainfall (0-1000mm), temperature (-10 to 40oC), humidity (0-100%), etc.\n",
    "- Standardition Convert all independent variables into the same scale (mean=0, std=1)\n",
    "- These differences in the ranges of initial features causes trouble to many machine learning models. For example, for the models that are based on distance computation, if one of the features has a broad range of values, the distance will be governed by this particular feature.\n",
    "- The example below use data from above:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "data_std = pd.DataFrame(scale(data3,axis=0, with_mean=True, with_std=True, copy=True))\n",
    "# axis used to compute the means and standard deviations along. If 0, independently standardize each feature, otherwise (if 1) standardize each sample."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2.2 Using scaling with predefine range\n",
    "Transform features by scaling each feature to a given range.\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "Formulation for this is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#By default, it scales for (0, 1) range\n",
    "data_scaler = pd.DataFrame(scaler.fit_transform(data3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2.3 Using Box-Cox Transformation\n",
    "- A [Box Cox](https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1964.tb00553.x) transformation is a transformation of a non-normal dependent variables into a normal shape.\n",
    "- Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n",
    "- The Box Cox transformation is named after statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and developed the technique.\n",
    "- BoxCox can only be applied to strictly positive values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "data_BxCx = pd.DataFrame(power_transform(data3,method=\"box-cox\"))\n",
    "data_BxCx.columns = data3.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2.4 Using Yeo Johnson Transformation\n",
    "While BoxCox only works with positive value, a more recent transformation method [Yeo Johnson](https://www.jstor.org/stable/2673623) can transform both positive and negative values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_yeo_johnson = sklearn.preprocessing.power_transform(data3,method=\"yeo-johnson\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.hist(data3[\"Ozone\"])\n",
    "ax1.set_title(\"Original probability\")\n",
    "ax1.set_xlabel('Ozone')\n",
    "ax1.set_ylabel('Count')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.hist(data_BxCx[\"Ozone\"])\n",
    "ax2.set_title(\"Box-Cox Transformation\")\n",
    "ax2.set_xlabel('Ozone')\n",
    "ax2.set_ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114884951-30a9e400-9dd4-11eb-9c42-4d108743a551.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Partition with Scikit-Learn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data partition: training and testing\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120378647-b1716080-c2ec-11eb-8693-60defbbad7e2.png)\n",
    "\n",
    "\n",
    "- In Machine Learning, it is mandatory to have training and testing set. Some time a verification set is also recommended.\n",
    "Here are some functions for spliting training/testing set in `sklearn`:\n",
    "\n",
    "- `train_test_split`: create series of test/training partitions\n",
    "- `Kfold` splits the data into k groups\n",
    "- `StratifiedKFold` splits the data into k groups based on a grouping factor.\n",
    "- `RepeatKfold`, `ShuffleSplit`, `LeaveOneOut`, `LeavePOut`\n",
    "\n",
    "Due to time constraints, we will only focus on `train_test_split` and  `KFolds`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Scikit-Learn data\n",
    "\n",
    "The `sklearn.datasets` package embeds some small sample datasets or toy [datasets](https://scikit-learn.org/stable/datasets.html)\n",
    "\n",
    "In this workshop, we are going to use some toy datasets but in real life, we can import any csv or table dataset:\n",
    "\n",
    "```\n",
    "For each toy dataset, there are 4 varibles:\n",
    "- **data**: numpy array of predictors/X\n",
    "- **target**: numpy array of predictant/target/y\n",
    "- **feature_names**: names of all predictors in X\n",
    "- **target_names**: names of all predictand in y\n",
    "```\n",
    "\n",
    "For example, we will load the California housing dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "print(data.data)\n",
    "print(data.target)\n",
    "print(data.feature_names)\n",
    "print(data.target_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can assign the variables for input and output data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Data spliting using train_test_split: **Single fold**\n",
    "Here we use `train_test_split` to randomly split 60% data for training and the rest for testing:\n",
    "    ![image](https://user-images.githubusercontent.com/43855029/114209883-22b81700-992d-11eb-83a4-c4ab1538a1e5.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.6,random_state=123)\n",
    "#random_state: int, similar to R set_seed function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Data spliting using `K-fold`\n",
    "- This is the Cross-validation approach.\n",
    "- This is a resampling process used to evaluate ML model on limited data sample.\n",
    "- The general procedure:\n",
    "- Shuffle data randomly\n",
    "- Split the data into **k** groups\n",
    "For each group:\n",
    "- Split into training & testing set\n",
    "- Fit a model on each group's training & testing set\n",
    "- Retain the evaluation score and summarize the skill of model\n",
    "\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/114211785-103edd00-992f-11eb-89d0-bbd7bd0c0178.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf10 = KFold(n_splits=10,shuffle=True,random_state=20)\n",
    "for train_index, test_index in kf10.split(data.target):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train) #Training the model, not running now\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(y_test, y_pred)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Metrics with Scikit-Learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Evaluation Metrics\n",
    "\n",
    "- Evaluation Metric is an essential part in any Machine Learning project.\n",
    "- It measures how good or bad is your Machine Learning model\n",
    "- Different Evaluation Metrics are used for Regression model (Continuous output) or Classification model (Categorical output).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Regression model Evaluation Metrics\n",
    "\n",
    "### 4.1.1 Correlation Coefficient (R) or Coefficient of Determination (R2):\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700259-72274900-c47f-11eb-8959-a4bbe4eafccc.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.r2_score(y_test,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 4.1.2 Root Mean Square Error (RMSE) or Mean Square Error (MSE)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700533-c5010080-c47f-11eb-8050-b1cd8c63746e.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_squared_error(y_test,y_pred,squared=False) # RMSE\n",
    "metrics.mean_squared_error(y_test,y_pred,squared=True) # MSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2. Classification Model Evaluation Metrics\n",
    "\n",
    "### 4.2.1 Confusion Matrix\n",
    "- A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n",
    "- You can learn more about Confusion Matrix [here](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)\n",
    "\n",
    "For binary output (classification problem with only 2 output type, also most popular):\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120687356-efe35880-c46f-11eb-950f-5feef237a4c1.png)\n",
    "\n",
    "### 4.2.2 Accuracy\n",
    "\n",
    "The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700619-dea24800-c47f-11eb-81c4-df090cad93da.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2.3 Precision\n",
    "\n",
    "Precision is the fraction of predicted positives events that are actually positive as shown below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700808-1c9f6c00-c480-11eb-9ec8-597d02a76a94.png)\n",
    "\n",
    "### 4.2.4 Recall\n",
    "\n",
    "Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120700754-07c2d880-c480-11eb-81e1-7c7926452346.png)\n",
    "\n",
    "\n",
    "### 4.2.5 F1 score\n",
    "\n",
    "The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120701061-6ee08d00-c480-11eb-9ab1-71d905e6a491.png)\n",
    "\n",
    "More information on Precision, Recall and F1 score can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics.precision_recall_fscore_support(y_test,y_pred,average='binary')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.6 AUC-ROC curve\n",
    "- ROC: Receiver Operating Characteristics:  probability curve\n",
    "- AUC: Area Under The Curve: represents the degree or measure of separability.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120698991-ccbfa580-c47d-11eb-9f11-6e2acb00d46d.png)\n",
    "\n",
    "- AUC = 1:   perfect prediction\n",
    "- AUC = 0.8: model has 80% chance to predict the right class\n",
    "    - AUC = 0.5: worst case, model has **NO** accuracy in prediction (random)\n",
    "- AUC = 0:   the model is actually reciprocating the classes\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/120699552-84ed4e00-c47e-11eb-8089-54158439ad6f.png)\n",
    "\n",
    "ROC Interpretation\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/133898061-2c7f5da6-c41b-41af-8a81-b65fef3c3184.png)\n",
    "\n",
    "Code to calculate FPR, TPR:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code to calculate AUC score:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = roc_auc_score(y_test,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will go into detail how to plot AUC-ROC curve in the next chapters with a classification problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Supervised Learning with continuous output\n",
    "For this session, we will use several Machine Learning algorithm  to work with continuous output the supervised learning problem.\n",
    "First of all, let's import the data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Preprocessing\n",
    "\n",
    "\n",
    "### 5.1.1 Import data\n",
    "\n",
    "\n",
    "Let use the **california housing** data in previous episodes:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Predictors/Input:\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "\n",
    "# Predictand/output:\n",
    "y = pd.DataFrame(data.target,columns=data.target_names)\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.2 Check missing data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there is no missing data, we move on to the next step:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.3 Split model into training & testing set with 60% for training:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.6,random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.4 Visualization the inputs and output:\n",
    "\n",
    "We can visualize the inputs and output data using pair plotting with seaborn package.\n",
    "\n",
    "Make sure that you install seaborn package in advance (Open the ML_SKLN Console (Not Notebook) and run this command:)\n",
    "\n",
    " `pip install seaborn`\n",
    "\n",
    "\n",
    "Once seaborn is installed, you can plot visualize the input/output data:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat([X_train,y_train], axis=1)\n",
    "sns.pairplot(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/154369953-8fcbc740-9d98-41b8-9c68-1f91149d59b2.png)\n",
    "\n",
    "Now the input data is ready for supervised learning model, let's select several ML algorithms to work with:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Machine Learning algorithm with Linear Regression\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 Train model using Linear Regression with 1 predictor (for example Medium Income)\n",
    "\n",
    "#### Fit a Linear model using LinearRegression model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model_linreg1 = LinearRegression().fit(pd.DataFrame(X_train['MedInc']),y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Apply trained model to testing data set and evaluate output using R-squared:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = model_linreg1.predict(pd.DataFrame(X_test['MedInc']))\n",
    "print(\"R2 is: %1.2f\" % metrics.r2_score(y_test,y_pred))\n",
    "print(\"RMSE is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that using 1 predictor/input, we obtain the output with corresponding R2 of 0.48 and RMSE = 0.84, which is not good enough. (The good R2 should be more than 0.7)\n",
    "Therefore, we change the approach, still using Linear Regression but with more inputs:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 Train model using Multi-Linear Regression (with 2 or more predictors)\n",
    "In this section, we will build the model with 4 inputs [\"MedInc\",\"HouseAge\",\"AveRooms\",\"Population\"]\n",
    "\n",
    "#### Fit the training set and predict using test set\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_linreg = LinearRegression().fit(X_train[[\"MedInc\",\"HouseAge\",\"AveRooms\",\"Population\"]],y_train)\n",
    "y_pred2 = model_linreg.predict(X_test[[\"MedInc\",\"HouseAge\",\"AveRooms\",\"Population\"]])\n",
    "\n",
    "print(\"R2 is: %1.2f\" % metrics.r2_score(y_test,y_pred2))\n",
    "print(\"RMSE is: %1.2f\"  % metrics.mean_squared_error(y_test,y_pred2,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Output is therefore better with smaller RMSE and higher R-squared:\n",
    "\n",
    "```\n",
    "R2 is: 0.52\n",
    "RMSE is: 0.80\n",
    "```\n",
    "\n",
    "Still the model outcome is not good enough, so we try another algorithm:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 Train model using Polynomial Regression\n",
    "\n",
    "We can slightly improve this by using Polynomial Regression\n",
    "![image](https://user-images.githubusercontent.com/43855029/115059030-f7e13c00-9eb3-11eb-9887-52461d7a87aa.png)\n",
    "\n",
    "#### Preprocessing: polynomial regression with `degree of freedom=2`\n",
    "the degree-2 polynomial features for 2 inputs (a & b) are [1, a, b, a^2, ab, b^2]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X[[\"MedInc\",\"HouseAge\",\"AveRooms\",\"Population\"]])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(X_poly),y, train_size=0.6,random_state=123)\n",
    "\n",
    "print(X_poly.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that using 4 inputs data, with Polynomial regression, we have 15 input variables [1\ta\tb\tc\td\ta2\tab\tac\tad\tb2\tbc\tbd\tc2\tcd\td2]\n",
    "\n",
    "#### Fit the new dataset and predict output:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_linreg_poly = LinearRegression().fit(X_train,y_train)\n",
    "y_pred_poly = model_linreg_poly.predict(X_test)\n",
    "\n",
    "print(\"R2 is: %1.2f \" % metrics.r2_score(y_test,y_pred_poly))\n",
    "print(\"RMSE is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred_poly,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is even better with R2 for testing data is 0.55 and lower RMSE.\n",
    "\n",
    "The **R2=0.55** shows improvement using polynomial regression!\n",
    "\n",
    "How about using more degrees of freedom?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Polynomial regression with `degree of freedom = 4`\n",
    "\n",
    "Can we improve the result with more degree of freedome? Let's try using df=4:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "X_poly = poly.fit_transform(X[[\"MedInc\",\"HouseAge\",\"AveRooms\",\"Population\"]])\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(pd.DataFrame(X_poly),y, train_size=0.6,random_state=123)\n",
    "\n",
    "model_linreg_poly4 = LinearRegression().fit(X_train4,y_train4)\n",
    "y_pred_poly4 = model_linreg_poly4.predict(X_test4)\n",
    "\n",
    "print(\"R2 for 4 dof is: %1.2f \" % metrics.r2_score(y_test4,y_pred_poly4))\n",
    "print(\"RMSE for 4 dof is: %1.2f\" % metrics.mean_squared_error(y_test4,y_pred_poly4,squared=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The R2 in sklearn can be negative, it arbitrarily means that the model is worse. More info on sklearn [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html).\n",
    "\n",
    "Why increasing the degree of freedom, my results getting worst?\n",
    "It's called OVERFITTING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Overfitting\n",
    "\n",
    "Overfitting occurs when we used lots of unesscessary input data for training process. It fits the training data so well that it is worse when applied to testing data:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153645935-b9eeebe5-424a-490a-aa95-006088a66b21.png)\n",
    "\n",
    "Exercise 1: Let use all dataset to train the data to see if using all input data, we have overfitting?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercise 2: Let's check the R2 and RMSE for training set using 2 and 4 degree of freedom to see if the 4 dof is better than 2 dof in fitting back to training data?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 Other Supervised ML algorithm for continuous data\n",
    "\n",
    "There are many other ML algorithm that helps to overcome the issue of overfitting, for example:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.1 Decision Tree\n",
    "\n",
    "- Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods.\n",
    "- Tree based methods empower predictive models with high accuracy, stability and ease of interpretation\n",
    "- Non-parametric and non-linear relationships\n",
    "- Types: Continuous (DecisionTreeRegressor) and Categorical (DecisionTreeClassifier)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153648313-da3a9a08-c4ad-48c9-bebd-df34f1651f98.png)\n",
    "\n",
    "Let use all data in this exercise, the Decision Tree algorithm for continuous output in sklearn is called **DecisionTreeRegressor**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.6,random_state=123)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_DT = DecisionTreeRegressor(max_depth=4).fit(X_train,y_train)\n",
    "y_pred_DT = model_DT.predict(X_test)\n",
    "\n",
    "print(\"R2 using Decision Tree is: %1.2f \" % metrics.r2_score(y_test,y_pred_DT))\n",
    "print(\"RMSE using Decision Tree is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred_DT,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see that Decision Tree helps to overcome the overfitting by trimming down the unnecessary input data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Visualization the Decision Tree:\n",
    "\n",
    "The following required graphviz model to be loaded when you requested for a Python Notebook.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153649826-000cc8ab-dfb9-43b7-b31c-26ecaf03d0a1.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(model_DT, out_file=None,\n",
    "                                filled=True, rounded=True,\n",
    "                                feature_names=data.feature_names,\n",
    "                                special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://user-images.githubusercontent.com/43855029/154372355-9660f717-0bd4-4f32-aaf1-24ae092fc1ff.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.2 Random Forest\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153650870-13494bba-d440-4006-b98a-6fb1509d10d5.png)\n",
    "\n",
    "\n",
    "- Random Forest is considered to be a panacea of all data science problems. On a funny note, when you can’t think of any algorithm (irrespective of situation), use random forest!\n",
    "- Opposite to Decision Tree, Random Forest use bootstrapping technique to grow multiple tree\n",
    "- Random Forest is a versatile machine learning method capable of performing both regression and classification tasks.\n",
    "- It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n",
    "- The end output of the model is like a black box and hence should be used judiciously.\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153650921-ecc70313-6e17-4bb6-92cb-bab11a39ab0c.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_RF = RandomForestRegressor(n_estimators=10).fit(X_train,y_train)\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "\n",
    "print(\"R2 using Random Forest is: %1.2f \" % metrics.r2_score(y_test,y_pred_RF))\n",
    "print(\"RMSE using Random Forest is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred_RF,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we use n=10 estimators (growing using n trees in the forest) and The output is much better:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 Ensemble Machine Learning\n",
    "\n",
    "- Ensemble is a method in Machine Learning that combine decision from several ML models to obtain optimum output.\n",
    "- Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives\n",
    "- The bonus point when applying both Bagging and Boosting in sklearn that they can be run in parallel!\n",
    "\n",
    "**Types of Ensembles:**\n",
    "\n",
    "There are 2 main types of Ensembles in ML:\n",
    "\n",
    "- Bagging: Boostrap Aggregation\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153652070-c067fc10-6322-49d1-92ed-b27532af11b6.png)\n",
    "\n",
    "Random Forest is considered Bagging Ensemble method!\n",
    "\n",
    "- Boosting: Boost the weak predictors\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153652096-4e93d213-58b9-4b27-88fa-e8b42a9cd6e5.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.5.1 Bagging with RandomForest\n",
    "\n",
    "We can apply Bagging to different ML algorithm like Linear Regression, Decision Tree, Random Forest, etc.\n",
    "Following are the syntax:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "model_RF = RandomForestRegressor()\n",
    "\n",
    "model_bag_RF = BaggingRegressor(base_estimator=model_RF, n_estimators=100,\n",
    "                                bootstrap=True, n_jobs=-1,\n",
    "                                random_state=123)\n",
    "\n",
    "model_bag_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bagRF = model_bag_RF.predict(X_test)\n",
    "\n",
    "print(\"R2 using Bagging Random Forest is: %1.2f \" % metrics.r2_score(y_test,y_pred_bagRF))\n",
    "print(\"RMSE using Baggin Random Forest is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred_bagRF,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that here we use n_estimators = 100 for bagging model (it grows 100 times the RandomForest model).\n",
    "The n_jobs=-1 means that it utilizes all the cores inside a compute nodes that we have\n",
    "\n",
    "And the output is very similar to RandomForest."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try with some Boosting Ensemble approach:\n",
    "\n",
    "### 5.5.2 Boosting with Adaboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model_ADA = AdaBoostRegressor(n_estimators=100, learning_rate=0.03).fit(X_train, y_train)\n",
    "y_pred_ADA = model_ADA.predict(X_test)\n",
    "\n",
    "print(\"R2 using Adaboost is: %1.2f \" % metrics.r2_score(y_test,y_pred_ADA))\n",
    "print(\"RMSE using Adaboost is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred_ADA,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is not as good as Bagging RF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.5.3 Gradient Boosting Machine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model_GBM = GradientBoostingRegressor(n_estimators=100).fit(X_train,y_train)\n",
    "y_pred_GBM = model_GBM.predict(X_test)\n",
    "\n",
    "print(\"R2 using GBM is: %1.2f \" % metrics.r2_score(y_test,y_pred_GBM))\n",
    "print(\"RMSE using GBM is: %1.2f\" % metrics.mean_squared_error(y_test,y_pred_GBM,squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is better than Adaboost."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Which is better in Ensemble? Bagging or Boosting?\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/153654625-d7efe94d-1fc4-4ee6-9b4b-f897f52a909e.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Ensemble overcome the limitation of using only single model\n",
    "- Between bagging and boosting, there is no better approach without trial & error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}